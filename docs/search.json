[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stat final project",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "project_details.html",
    "href": "project_details.html",
    "title": "Overview of the Re-visualization Project",
    "section": "",
    "text": "Suicide is the act of intentionally causing one’s own death. It can be due to many conditions or the situations. It includes Mental disorders, physical disorders, and substance abuse are the risk factors. Suicides resulted in 828,000 deaths globally in 2015, an increase from 712,000 deaths in 1990. This makes suicide the 10th leading cause of death worldwide. Every death from suicide is a tragedy.\nThe below is the Visualization on Suicides by Saloni Dattani, Lucas Rodes-Guirao, Hannah Ritchie, Max Roser, and Esteban Ortiz-Ospina. The research shows that suicide rates can be reduced with greater understanding and support. To do that the researchers considered or recognized suicide as a public health problem, and people should know that it can be prevented and its rates can be reduced.\n\n\n\n\n\nSuicide rates vary around the world:\nSuicide rates vary widely between the countries. The given visualization depicts the data of annual suicide rates per 100,000 people from 1950 to 2022, across various countries. Researchers used line graph to predict the data.\n\nX-axis represents the years from 1950 to 2022 and y-axis represents the suicide rate per 100000 people, ranging from 0 to 40. It also says that higher the value, the greater will be the number of suicide rates.\nEach line of the graph represents the countries. The countries which have higher suicide rates are represented on the top. The legends taken are countries.\n\nObservations:\n\nThere is a wide range of variations between the countries. Countries like Lithuania, South Korea shows the highest suicide rates, as indicated by their position near the top of the graph.\nSome countries shows large fluctuations in the suicide rates while other countries shows the constant rate throughout the years.\nIt also says that suicide deaths are under-reported in many countries due to social stigma and culture or legal concerns means that actual rates can be higher than the reported rates.\nThe data is collected based on the data listed in the death certificates. It can impact the accuracy of the data\nThe data is adjusted for age standardization allowing a fair comparison between the countries with different age structures, ensuring that population age distribution doesn’t skew the data.\n\n\n\n\nMore number of lines: The graph contains a huge number of lines which are representing the countries. This creates a messy graph it is very difficult to predict the data immediately as we look into the graph.\nColor Categorization: All the countries represented with different colors but for some countries there are distinct colors where it will be very difficult to categorize the data. There are similar colors in for different countries. We can use more contrasting colors to represent the data or we can group the colors into regions or categories.\nInteractive Labeling: With so many lines we cannot identify the particular country instantly and it is impossible to find the particular country and there are all the countries mentioned in the legend where it is impossible to identify the specific country. Hence we can use interactive Labeling for highlighting the particular country.\nNo Highlights on the key insights: All the lines in the graph are in equal size where there is no differentiation between the countries. We can highlight the countries which have highest suicide rates and lowest suicide rates with different dimensions of the lines.\n\n\nThe above visualization tells us about the reported suicide rates by age in the United States.\nObservations:\nIt explains the breakdown for the rate of suicides for different age groups like children, adults. The data highlights the trends such as the increasing or decreasing risk of suicide within the specific age over time and across different regions.\n\nIt shows the data for the suicide rates per 100,000 people across different age groups. Age specific data usually reveals trends showing which age group are more vulnerable to suicide in different regions.\nAccording to the graph it predicts that the old generation people have the higher suicide rates (Age between 80-84). The age between 15-19 suicidal rates are less. But there is growing concern about suicidal rates in young adults particularly due to health conditions and mental stress.\n\n\n\n\n\nMore number of lines: The graph contains a huge number of lines which are representing the different age groups. This creates a messy graph it is very difficult to predict the data immediately as we look into the graph.\nColor Categorization: All the age groups are represented with different colors but for some there are distinct colors where it will be very difficult to categorize the data. There are similar colors for different age groups. We can use more contrasting colors to represent the data or we can group the colors into categories or age groups.\nLegend: The legend have too many entries where it is difficult for the user to identify the particular data of the age group in a particular year. Viewers must constantly shift their focus on the legend and the graph simultaneously where it would be difficult for predicting the exact information.\nLack of data insights: There is no contextual information or annotations on the graph to explain significant spikes, trends or sudden drops in the suicide rates for certain age groups.\nInteractive Labeling: Adding the interactvite labeling helps to improve the readability."
  },
  {
    "objectID": "project_details.html#introduction",
    "href": "project_details.html#introduction",
    "title": "Overview of the Re-visualization Project",
    "section": "Introduction:",
    "text": "Introduction:\nSuicide is the act of intentionally causing one’s own death. It can be due to many conditions or the situations. It includes Mental disorders, physical disorders, and substance abuse are the risk factors. Suicides resulted in 828,000 deaths globally in 2015, an increase from 712,000 deaths in 1990. This makes suicide the 10th leading cause of death worldwide. Every death from suicide is a tragedy.\nThe below is the Visualization on Suicides by Saloni Dattani, Lucas Rodes-Guirao, Hannah Ritchie, Max Roser, and Esteban Ortiz-Ospina. The research shows that suicide rates can be reduced with greater understanding and support. To do that the researchers considered or recognized suicide as a public health problem, and people should know that it can be prevented and its rates can be reduced.\n\nOLD VISUALIZATION:\n\n\n\nSuicide rates vary around the world:\nSuicide rates vary widely between the countries. The given visualization depicts the data of annual suicide rates per 100,000 people from 1950 to 2022, across various countries. Researchers used line graph to predict the data.\n\nX-axis represents the years from 1950 to 2022 and y-axis represents the suicide rate per 100000 people, ranging from 0 to 40. It also says that higher the value, the greater will be the number of suicide rates.\nEach line of the graph represents the countries. The countries which have higher suicide rates are represented on the top. The legends taken are countries.\n\nObservations:\n\nThere is a wide range of variations between the countries. Countries like Lithuania, South Korea shows the highest suicide rates, as indicated by their position near the top of the graph.\nSome countries shows large fluctuations in the suicide rates while other countries shows the constant rate throughout the years.\nIt also says that suicide deaths are under-reported in many countries due to social stigma and culture or legal concerns means that actual rates can be higher than the reported rates.\nThe data is collected based on the data listed in the death certificates. It can impact the accuracy of the data\nThe data is adjusted for age standardization allowing a fair comparison between the countries with different age structures, ensuring that population age distribution doesn’t skew the data.\n\n\nBad Visualization Predictions:\n\nMore number of lines: The graph contains a huge number of lines which are representing the countries. This creates a messy graph it is very difficult to predict the data immediately as we look into the graph.\nColor Categorization: All the countries represented with different colors but for some countries there are distinct colors where it will be very difficult to categorize the data. There are similar colors in for different countries. We can use more contrasting colors to represent the data or we can group the colors into regions or categories.\nInteractive Labeling: With so many lines we cannot identify the particular country instantly and it is impossible to find the particular country and there are all the countries mentioned in the legend where it is impossible to identify the specific country. Hence we can use interactive Labeling for highlighting the particular country.\nNo Highlights on the key insights: All the lines in the graph are in equal size where there is no differentiation between the countries. We can highlight the countries which have highest suicide rates and lowest suicide rates with different dimensions of the lines.\n\n\nThe above visualization tells us about the reported suicide rates by age in the United States.\nObservations:\nIt explains the breakdown for the rate of suicides for different age groups like children, adults. The data highlights the trends such as the increasing or decreasing risk of suicide within the specific age over time and across different regions.\n\nIt shows the data for the suicide rates per 100,000 people across different age groups. Age specific data usually reveals trends showing which age group are more vulnerable to suicide in different regions.\nAccording to the graph it predicts that the old generation people have the higher suicide rates (Age between 80-84). The age between 15-19 suicidal rates are less. But there is growing concern about suicidal rates in young adults particularly due to health conditions and mental stress.\n\n\n\nBad Visualization Predictions:\n\nMore number of lines: The graph contains a huge number of lines which are representing the different age groups. This creates a messy graph it is very difficult to predict the data immediately as we look into the graph.\nColor Categorization: All the age groups are represented with different colors but for some there are distinct colors where it will be very difficult to categorize the data. There are similar colors for different age groups. We can use more contrasting colors to represent the data or we can group the colors into categories or age groups.\nLegend: The legend have too many entries where it is difficult for the user to identify the particular data of the age group in a particular year. Viewers must constantly shift their focus on the legend and the graph simultaneously where it would be difficult for predicting the exact information.\nLack of data insights: There is no contextual information or annotations on the graph to explain significant spikes, trends or sudden drops in the suicide rates for certain age groups.\nInteractive Labeling: Adding the interactvite labeling helps to improve the readability."
  },
  {
    "objectID": "project_details.html#re-visualizations",
    "href": "project_details.html#re-visualizations",
    "title": "Overview of the Re-visualization Project",
    "section": "Re-Visualizations:",
    "text": "Re-Visualizations:\n\nAccording to the above research and bad visualizations found we have made some changes and re-visualized the data as below:\nEach map or graph in this project displays suicide rates per 100,000 people to enhance the clarity and effectiveness of the visualization and this is the standard that data analysts generally follow while visualizing death related data."
  },
  {
    "objectID": "project_details.html#average-suicidal-rates-by-country-from-1950-to-2022",
    "href": "project_details.html#average-suicidal-rates-by-country-from-1950-to-2022",
    "title": "Overview of the Re-visualization Project",
    "section": "Average Suicidal Rates By Country from 1950 to 2022:",
    "text": "Average Suicidal Rates By Country from 1950 to 2022:\nThe map below illustrates the average suicide rates by country from 1950 to 2022, broken down by different age groups such as children, young adults, and adults across all nations.\nIn the previous visualization, the data was presented in a line graph for all countries, resulting in a cluttered and hard-to-read display. To improve clarity, we have re-visualized the data by focusing on the average suicide rates from 1950 to 2022 using a world map. In the provided dataset, we calculated the average suicide rates over the years and made predictions based on that data. The world map offers an easier and more intuitive way to interpret the data. This updated map is also interactive, allowing users to highlight specific parameters and explore the average range of deaths by suicide more flexibly.\nBased on the predictions shown in the map, Russia has the highest average suicide rates."
  },
  {
    "objectID": "project_source_code.html",
    "href": "project_source_code.html",
    "title": "Project Source Code",
    "section": "",
    "text": "# Load required libraries\n\nsuppressWarnings({\n library(tidyverse)\nlibrary(ggplot2)\nlibrary(plotly)  \nlibrary(maps)\nlibrary(leaflet)\nlibrary(readr)\n\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(viridis) \nlibrary(rnaturalearth)  # For loading world map data\n\n\n})\n\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Load the data\nroad_casualities &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Data Cleaning Steps\n# 1. Remove rows with missing or NA values in important columns\nroad_casualities &lt;- road_casualities %&gt;%\n  filter(!is.na(accident_severity), \n         !is.na(weather_conditions), \n         !is.na(road_surface_conditions))\n\n# 2. Ensure the accident_severity column has valid values (1, 2, 3)\nroad_casualities &lt;- road_casualities %&gt;%\n  filter(accident_severity %in% c(1, 2, 3))\n\n# 3. Convert necessary columns to factors\nroad_casualities$accident_severity &lt;- factor(road_casualities$accident_severity, \n                                 levels = c(1, 2, 3),\n                                 labels = c(\"Life-Threatening\", \"Significant\", \"Mild\"))\nroad_casualities$weather_conditions &lt;- as.factor(road_casualities$weather_conditions)\nroad_casualities$road_surface_conditions &lt;- as.factor(road_casualities$road_surface_conditions)\nroad_casualities$urban_or_rural_area &lt;- as.factor(road_casualities$urban_or_rural_area)\n\n# 4. Remove duplicates if any\nroad_casualities &lt;- road_casualities %&gt;% distinct()\n\n\n# Visualization with Cleaned Data\n# Assign updated custom colors for each severity level\nseverity_colors &lt;- c(\"Life-Threatening\" = \"red\", \n                     \"Significant\" = \"darkorange\", \n                     \"Mild\" = \"darkgreen\")\n\n# Create an Interactive Histogram with Updated Colors\nhistogram &lt;- ggplot(road_casualities, aes(x = accident_severity, fill = accident_severity)) +\n  geom_bar(alpha = 0.7, color = \"black\") +\n  scale_fill_manual(values = severity_colors) +\n  labs(title = \"Distribution of Accident Severity\",\n       x = \"Accident Severity\",\n       y = \"Count\") \n\n# Convert to interactive using plotly\ninteractive_histogram &lt;- ggplotly(histogram, tooltip = c(\"count\", \"x\"))\n\n# Display the interactive histogram\ninteractive_histogram\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Load dataset\nroad_casualities &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Convert time column to a usable format (assuming time is in 24-hour format)\nroad_casualities$time_of_day &lt;- as.numeric(substr(road_casualities$time, 1, 2))\n\n# Define time bands based on STATS20 guidance\nroad_casualities$time_band &lt;- cut(\n  road_casualities$time_of_day,\n  breaks = c(-1, 5, 9, 15, 19, 23),\n  labels = c(\"Night (Midnight to 5 AM)\", \n             \"Morning Rush Hour\", \n             \"Daytime\", \n             \"Evening Rush Hour\", \n             \"Night (8 PM to 11 PM)\")\n)\n\n# Aggregate the data by time bands\ntime_band_summary &lt;- road_casualities %&gt;%\n  group_by(time_band) %&gt;%\n  summarise(total_accidents = n()) %&gt;%\n  arrange(desc(total_accidents))\n\n# Create a ggplot object with gradient colors\ntime_band_plot &lt;- ggplot(time_band_summary, aes(x = time_band, y = total_accidents, fill = total_accidents)) +\n  geom_bar(stat = \"identity\", color = \"black\", alpha = 0.8) +\n  labs(\n    title = \"Accidents by Time Bands (STATS20)\",\n    x = \"Time Band\",\n    y = \"Number of Accidents\"\n  ) +\n  scale_fill_gradient(low = \"pink\", high = \"red\", name = \"Total Accidents\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Convert the plot to interactive using plotly\ninteractive_time_band_plot &lt;- ggplotly(time_band_plot, tooltip = c(\"x\", \"y\"))\n\n# Display the interactive plot\ninteractive_time_band_plot\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\n\n# Define light condition mapping (STATS20)\nlight_conditions_map &lt;- c(\n  \"1\" = \"Daylight\",\n  \"4\" = \"Dark (street lights present and lit)\",\n  \"5\" = \"Dark (street lights present but unlit)\",\n  \"6\" = \"Dark (no street lights)\",\n  \"7\" = \"Other\"\n)\n\n# Map light conditions\nroad_casualities$light_conditions &lt;- as.factor(\n  light_conditions_map[as.character(road_casualities$light_conditions)]\n)\n\n# Ensure weather conditions are mapped\nweather_conditions_map &lt;- c(\n  \"1\" = \"Fine without high winds\",\n  \"2\" = \"Raining without high winds\",\n  \"3\" = \"Snowing without high winds\",\n  \"4\" = \"Fine with high winds\",\n  \"5\" = \"Raining with high winds\",\n  \"6\" = \"Snowing with high winds\",\n  \"7\" = \"Fog or mist\",\n  \"8\" = \"Other\",\n  \"9\" = \"Unknown\"\n)\n\nroad_casualities$weather_conditions &lt;- as.factor(\n  weather_conditions_map[as.character(road_casualities$weather_conditions)]\n)\n\n# Create summary data for heatmap\nheatmap_data &lt;- road_casualities %&gt;%\n  group_by(weather_conditions, light_conditions) %&gt;%\n  summarise(total_accidents = n(), .groups = \"drop\")\n\n# Create the heatmap with data labels and contrasting colors\nheatmap_plot &lt;- ggplot(heatmap_data, aes(x = weather_conditions, y = light_conditions, fill = total_accidents)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = total_accidents), color = \"black\", size = 3) +  # Add data labels\n  scale_fill_gradient(low = \"#f9f9f9\", high = \"#d73027\", name = \"Total Accidents\") +\n  labs(\n    title = \"Accidents by Weather and Light Conditions\",\n    x = \"Weather Conditions\",\n    y = \"Light Conditions\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),\n    axis.text.y = element_text(size = 10),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 8)\n  )\n\n# Make the heatmap interactive\ninteractive_heatmap &lt;- ggplotly(heatmap_plot, tooltip = c(\"x\", \"y\", \"fill\"))\ninteractive_heatmap\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\n\n# Load the dataset\nroad_casualities &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Define weather condition mapping\nweather_conditions_map &lt;- c(\n  \"1\" = \"Fine without high winds\",\n  \"2\" = \"Raining without high winds\",\n  \"3\" = \"Snowing without high winds\",\n  \"4\" = \"Fine with high winds\",\n  \"5\" = \"Raining with high winds\",\n  \"6\" = \"Snowing with high winds\",\n  \"7\" = \"Fog or mist\",\n  \"8\" = \"Other\",\n  \"9\" = \"Unknown\"\n)\n\n# Map weather conditions\nroad_casualities$weather_conditions_desc &lt;- as.factor(weather_conditions_map[as.character(road_casualities$weather_conditions)])\n\n# Summarize the data to get accident counts by weather condition\nweather_summary &lt;- road_casualities %&gt;%\n  group_by(weather_conditions_desc) %&gt;%\n  summarise(total_accidents = n(), .groups = \"drop\")\n\n# Add a clean label for the tooltip\nweather_summary$tooltip_label &lt;- paste0(\n  \"Weather: \", weather_summary$weather_conditions_desc, \n  \"&lt;br&gt;Total Accidents: \", weather_summary$total_accidents\n)\n\n# Create a bar plot with dynamic red shading\nweather_plot &lt;- ggplot(weather_summary, aes(\n  x = reorder(weather_conditions_desc, -total_accidents), \n  y = total_accidents, \n  fill = total_accidents, \n  text = tooltip_label  # Use clean tooltip labels\n)) +\n  geom_bar(stat = \"identity\", color = \"black\", alpha = 0.8) +\n  scale_fill_gradient(low = \"#FFC1C1\", high = \"#8B0000\", name = \"Total Accidents\") +\n  labs(\n    title = \"Accidents by Weather Conditions\",\n    x = \"Weather Conditions\",\n    y = \"Number of Accidents\"\n  ) \n\n# Make it interactive and use the clean tooltip\ninteractive_weather_plot &lt;- ggplotly(weather_plot, tooltip = \"text\")\ninteractive_weather_plot\n\n\n\n\n\n\n# Load the dataset\nroad_casualities &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\n\n# Define STATS19 road surface condition mapping\nroad_conditions_map &lt;- c(\n  \"1\" = \"Dry\",\n  \"2\" = \"Wet or damp\",\n  \"3\" = \"Snow\",\n  \"4\" = \"Frost or ice\",\n  \"5\" = \"Flood\",\n  \"6\" = \"Oil or diesel\",\n  \"7\" = \"Mud\",\n  \"8\" = \"Other\",\n  \"9\" = \"Unknown\"\n)\n\n# Step 1: Filter valid road_surface_conditions values (1 to 9)\nroad_casualities &lt;- road_casualities %&gt;%\n  filter(road_surface_conditions %in% c(1:9))\n\n# Step 2: Map road conditions to descriptions\nroad_casualities$road_conditions_desc &lt;- as.factor(\n  road_conditions_map[as.character(road_casualities$road_surface_conditions)]\n)\n\n# Debugging: Check unique values in road_conditions_desc\nprint(\"Mapped descriptions:\")\n\n[1] \"Mapped descriptions:\"\n\nprint(unique(road_casualities$road_conditions_desc))\n\n[1] Wet or damp  Dry          Unknown      Frost or ice Snow        \n[6] Flood       \nLevels: Dry Flood Frost or ice Snow Unknown Wet or damp\n\n# Step 3: Aggregate data by road surface condition\nroad_summary &lt;- road_casualities %&gt;%\n  group_by(road_conditions_desc) %&gt;%\n  summarise(total_accidents = n()) %&gt;%\n  arrange(desc(total_accidents))\n\n# Debugging: Check aggregated data\nprint(\"Aggregated road surface condition data:\")\n\n[1] \"Aggregated road surface condition data:\"\n\nprint(road_summary)\n\n# A tibble: 6 × 2\n  road_conditions_desc total_accidents\n  &lt;fct&gt;                          &lt;int&gt;\n1 Dry                            72752\n2 Wet or damp                    26944\n3 Unknown                         1617\n4 Frost or ice                    1461\n5 Snow                             241\n6 Flood                            179\n\n# Step 4: Create the bar plot\nroad_plot &lt;- ggplot(road_summary, aes(\n  x = reorder(road_conditions_desc, -total_accidents), \n  y = total_accidents, \n  fill = road_conditions_desc, \n  text = paste0(road_conditions_desc, \": \", total_accidents, \" accidents\")\n)) +\n  geom_bar(stat = \"identity\", alpha = 0.8, color = \"black\") +\n  labs(\n    title = \"Distribution of Accidents by Road Surface Conditions (STATS19)\",\n    x = \"Road Surface Conditions\",\n    y = \"Number of Accidents\"\n  ) +\n  scale_fill_manual(values = c(\n    \"Dry\" = \"red\",\n    \"Wet or damp\" = \"#73a2c6\",\n    \"Snow\" = \"#1b9e77\",\n    \"Frost or ice\" = \"#d95f02\",\n    \"Flood\" = \"#7570b3\",\n    \"Oil or diesel\" = \"#e7298a\",\n    \"Mud\" = \"#66a61e\",\n    \"Other\" = \"#e6ab02\",\n    \"Unknown\" = \"#a6761d\"\n  )) \n\n# Convert the plot to an interactive plot\ninteractive_road_plot &lt;- ggplotly(road_plot, tooltip = \"text\")\n\n# Display the interactive plot\ninteractive_road_plot\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(caret)\n# Load necessary libraries\nlibrary(nnet)  # For multinomial logistic regression\nlibrary(caret) # For data partitioning\n\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n\n# Select relevant columns and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, c(\"accident_severity\", \"weather_conditions\")])\n\n# Convert columns to factors\ndata_cleaned$accident_severity &lt;- as.factor(data_cleaned$accident_severity)\ndata_cleaned$weather_conditions &lt;- as.numeric(data_cleaned$weather_conditions)\n\n# Split the dataset into training and testing sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(data_cleaned$accident_severity, p = 0.8, list = FALSE)\ntrain_data &lt;- data_cleaned[train_index, ]\ntest_data &lt;- data_cleaned[-train_index, ]\n\n# Train multinomial logistic regression model\nmultinom_model &lt;- multinom(accident_severity ~ weather_conditions, data = train_data)\n\n# weights:  9 (4 variable)\ninitial  value 91633.053773 \niter  10 value 50406.679444\nfinal  value 50392.493956 \nconverged\n\n# Summarize the model\nsummary(multinom_model)\n\nCall:\nmultinom(formula = accident_severity ~ weather_conditions, data = train_data)\n\nCoefficients:\n  (Intercept) weather_conditions\n2     2.64414         0.06126746\n3     3.76240         0.12285242\n\nStd. Errors:\n  (Intercept) weather_conditions\n2  0.04240724         0.02156981\n3  0.04154713         0.02120177\n\nResidual Deviance: 100785 \nAIC: 100793 \n\n# Make predictions\npredictions &lt;- predict(multinom_model, newdata = test_data)\n\n# Evaluate the model\nconfusion_matrix &lt;- table(test_data$accident_severity, predictions)\nprint(\"Confusion Matrix:\")\n\n[1] \"Confusion Matrix:\"\n\nprint(confusion_matrix)\n\n   predictions\n        1     2     3\n  1     0     0   304\n  2     0     0  4687\n  3     0     0 15859\n\n# Calculate accuracy\naccuracy &lt;- sum(diag(confusion_matrix)) / sum(confusion_matrix)\nprint(paste(\"Accuracy:\", round(accuracy, 4)))\n\n[1] \"Accuracy: 0.7606\"\n\n\n\n# Load necessary libraries\nlibrary(nnet)  # For multinomial logistic regression\nlibrary(caret) # For data partitioning\nlibrary(pROC)  # For AUC and ROC curves\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Select relevant columns and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, c(\"accident_severity\", \"weather_conditions\")])\n\n# Convert columns to factors and numeric types\ndata_cleaned$accident_severity &lt;- as.factor(data_cleaned$accident_severity)\ndata_cleaned$weather_conditions &lt;- as.numeric(data_cleaned$weather_conditions)\n\n# Split the dataset into training and testing sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(data_cleaned$accident_severity, p = 0.8, list = FALSE)\ntrain_data &lt;- data_cleaned[train_index, ]\ntest_data &lt;- data_cleaned[-train_index, ]\n\n# Train multinomial logistic regression model\nmultinom_model &lt;- multinom(accident_severity ~ weather_conditions, data = train_data)\n\n# weights:  9 (4 variable)\ninitial  value 91633.053773 \niter  10 value 50406.679444\nfinal  value 50392.493956 \nconverged\n\n# Predict probabilities for the test data\nprobabilities &lt;- predict(multinom_model, newdata = test_data, type = \"probs\")\n\n# Create one-vs-all ROC curves for each class\nroc_curve_1 &lt;- roc(as.numeric(test_data$accident_severity == 1), probabilities[, 1], plot = TRUE, col = \"red\", main = \"ROC Curve for Multinomial Logistic Regression\")\nroc_curve_2 &lt;- roc(as.numeric(test_data$accident_severity == 2), probabilities[, 2], plot = TRUE, col = \"blue\", add = TRUE)\nroc_curve_3 &lt;- roc(as.numeric(test_data$accident_severity == 3), probabilities[, 3], plot = TRUE, col = \"green\", add = TRUE)\n\n# Add legend\nlegend(\"bottomright\", legend = c(\"Class 1 (Fatal)\", \"Class 2 (Serious)\", \"Class 3 (Slight)\"),\n       col = c(\"red\", \"blue\", \"green\"), lwd = 2)\n\n\n\n\n\n\n\n# Compute AUC for each class\nauc_1 &lt;- auc(roc_curve_1)\nauc_2 &lt;- auc(roc_curve_2)\nauc_3 &lt;- auc(roc_curve_3)\n\n\n# Load necessary libraries\n# Load necessary libraries\nlibrary(randomForest)\nlibrary(pROC)\nlibrary(caret)\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Select relevant features and target variable\nfeatures &lt;- c(\"weather_conditions\", \"number_of_vehicles\", \"road_surface_conditions\", \"urban_or_rural_area\")\ntarget &lt;- \"accident_severity\"\n\n# Filter data to include only relevant columns and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, c(features, target)])\n\n# Convert target variable to factor for classification\ndata_cleaned$accident_severity &lt;- as.factor(data_cleaned$accident_severity)\n\n# Split data into training and testing sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(data_cleaned$accident_severity, p = 0.8, list = FALSE)\ntrain_data &lt;- data_cleaned[train_index, ]\ntest_data &lt;- data_cleaned[-train_index, ]\n\n# Train a Random Forest model\nrf_model &lt;- randomForest(\n  accident_severity ~ .,\n  data = train_data,\n  ntree = 100,\n  importance = TRUE\n)\n\n# Print model summary\nprint(rf_model)\n\n\nCall:\n randomForest(formula = accident_severity ~ ., data = train_data,      ntree = 100, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 23.95%\nConfusion matrix:\n  1  2     3  class.error\n1 0  0  1218 1.0000000000\n2 0  6 18745 0.9996800171\n3 0 15 63424 0.0002364476\n\n# Predict probabilities on the test set\nrf_probabilities &lt;- predict(rf_model, newdata = test_data, type = \"prob\")\n\n# Draw AUC Curve for each class using one-vs-all approach\nauc_values &lt;- list()\nfor (i in 1:ncol(rf_probabilities)) {\n  class_label &lt;- colnames(rf_probabilities)[i]\n  roc_curve &lt;- roc(as.numeric(test_data$accident_severity == class_label), \n                   rf_probabilities[, i], \n                   plot = TRUE, \n                   main = paste(\"ROC Curve for Class\", class_label), \n                   col = i)\n  auc_values[[class_label]] &lt;- auc(roc_curve)\n  print(paste(\"AUC for Class\", class_label, \":\", round(auc(roc_curve), 4)))\n}\n\n\n\n\n\n\n\n\n[1] \"AUC for Class 1 : 0.4996\"\n\n\n\n\n\n\n\n\n\n[1] \"AUC for Class 2 : 0.5012\"\n\n\n\n\n\n\n\n\n\n[1] \"AUC for Class 3 : 0.5012\"\n\n# Visualize feature importance\nvarImpPlot(rf_model)"
  },
  {
    "objectID": "project_source_code.html#loading-libraries",
    "href": "project_source_code.html#loading-libraries",
    "title": "Project Source Code",
    "section": "",
    "text": "# Load required libraries\n\nsuppressWarnings({\n library(tidyverse)\nlibrary(ggplot2)\nlibrary(plotly)  \nlibrary(maps)\nlibrary(leaflet)\nlibrary(readr)\n\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(viridis) \nlibrary(rnaturalearth)  # For loading world map data\n\n\n})\n\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Load the data\nroad_casualities &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Data Cleaning Steps\n# 1. Remove rows with missing or NA values in important columns\nroad_casualities &lt;- road_casualities %&gt;%\n  filter(!is.na(accident_severity), \n         !is.na(weather_conditions), \n         !is.na(road_surface_conditions))\n\n# 2. Ensure the accident_severity column has valid values (1, 2, 3)\nroad_casualities &lt;- road_casualities %&gt;%\n  filter(accident_severity %in% c(1, 2, 3))\n\n# 3. Convert necessary columns to factors\nroad_casualities$accident_severity &lt;- factor(road_casualities$accident_severity, \n                                 levels = c(1, 2, 3),\n                                 labels = c(\"Life-Threatening\", \"Significant\", \"Mild\"))\nroad_casualities$weather_conditions &lt;- as.factor(road_casualities$weather_conditions)\nroad_casualities$road_surface_conditions &lt;- as.factor(road_casualities$road_surface_conditions)\nroad_casualities$urban_or_rural_area &lt;- as.factor(road_casualities$urban_or_rural_area)\n\n# 4. Remove duplicates if any\nroad_casualities &lt;- road_casualities %&gt;% distinct()\n\n\n# Visualization with Cleaned Data\n# Assign updated custom colors for each severity level\nseverity_colors &lt;- c(\"Life-Threatening\" = \"red\", \n                     \"Significant\" = \"darkorange\", \n                     \"Mild\" = \"darkgreen\")\n\n# Create an Interactive Histogram with Updated Colors\nhistogram &lt;- ggplot(road_casualities, aes(x = accident_severity, fill = accident_severity)) +\n  geom_bar(alpha = 0.7, color = \"black\") +\n  scale_fill_manual(values = severity_colors) +\n  labs(title = \"Distribution of Accident Severity\",\n       x = \"Accident Severity\",\n       y = \"Count\") \n\n# Convert to interactive using plotly\ninteractive_histogram &lt;- ggplotly(histogram, tooltip = c(\"count\", \"x\"))\n\n# Display the interactive histogram\ninteractive_histogram\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Load dataset\nroad_casualities &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Convert time column to a usable format (assuming time is in 24-hour format)\nroad_casualities$time_of_day &lt;- as.numeric(substr(road_casualities$time, 1, 2))\n\n# Define time bands based on STATS20 guidance\nroad_casualities$time_band &lt;- cut(\n  road_casualities$time_of_day,\n  breaks = c(-1, 5, 9, 15, 19, 23),\n  labels = c(\"Night (Midnight to 5 AM)\", \n             \"Morning Rush Hour\", \n             \"Daytime\", \n             \"Evening Rush Hour\", \n             \"Night (8 PM to 11 PM)\")\n)\n\n# Aggregate the data by time bands\ntime_band_summary &lt;- road_casualities %&gt;%\n  group_by(time_band) %&gt;%\n  summarise(total_accidents = n()) %&gt;%\n  arrange(desc(total_accidents))\n\n# Create a ggplot object with gradient colors\ntime_band_plot &lt;- ggplot(time_band_summary, aes(x = time_band, y = total_accidents, fill = total_accidents)) +\n  geom_bar(stat = \"identity\", color = \"black\", alpha = 0.8) +\n  labs(\n    title = \"Accidents by Time Bands (STATS20)\",\n    x = \"Time Band\",\n    y = \"Number of Accidents\"\n  ) +\n  scale_fill_gradient(low = \"pink\", high = \"red\", name = \"Total Accidents\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Convert the plot to interactive using plotly\ninteractive_time_band_plot &lt;- ggplotly(time_band_plot, tooltip = c(\"x\", \"y\"))\n\n# Display the interactive plot\ninteractive_time_band_plot\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\n\n# Define light condition mapping (STATS20)\nlight_conditions_map &lt;- c(\n  \"1\" = \"Daylight\",\n  \"4\" = \"Dark (street lights present and lit)\",\n  \"5\" = \"Dark (street lights present but unlit)\",\n  \"6\" = \"Dark (no street lights)\",\n  \"7\" = \"Other\"\n)\n\n# Map light conditions\nroad_casualities$light_conditions &lt;- as.factor(\n  light_conditions_map[as.character(road_casualities$light_conditions)]\n)\n\n# Ensure weather conditions are mapped\nweather_conditions_map &lt;- c(\n  \"1\" = \"Fine without high winds\",\n  \"2\" = \"Raining without high winds\",\n  \"3\" = \"Snowing without high winds\",\n  \"4\" = \"Fine with high winds\",\n  \"5\" = \"Raining with high winds\",\n  \"6\" = \"Snowing with high winds\",\n  \"7\" = \"Fog or mist\",\n  \"8\" = \"Other\",\n  \"9\" = \"Unknown\"\n)\n\nroad_casualities$weather_conditions &lt;- as.factor(\n  weather_conditions_map[as.character(road_casualities$weather_conditions)]\n)\n\n# Create summary data for heatmap\nheatmap_data &lt;- road_casualities %&gt;%\n  group_by(weather_conditions, light_conditions) %&gt;%\n  summarise(total_accidents = n(), .groups = \"drop\")\n\n# Create the heatmap with data labels and contrasting colors\nheatmap_plot &lt;- ggplot(heatmap_data, aes(x = weather_conditions, y = light_conditions, fill = total_accidents)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = total_accidents), color = \"black\", size = 3) +  # Add data labels\n  scale_fill_gradient(low = \"#f9f9f9\", high = \"#d73027\", name = \"Total Accidents\") +\n  labs(\n    title = \"Accidents by Weather and Light Conditions\",\n    x = \"Weather Conditions\",\n    y = \"Light Conditions\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),\n    axis.text.y = element_text(size = 10),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 8)\n  )\n\n# Make the heatmap interactive\ninteractive_heatmap &lt;- ggplotly(heatmap_plot, tooltip = c(\"x\", \"y\", \"fill\"))\ninteractive_heatmap\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\n\n# Load the dataset\nroad_casualities &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Define weather condition mapping\nweather_conditions_map &lt;- c(\n  \"1\" = \"Fine without high winds\",\n  \"2\" = \"Raining without high winds\",\n  \"3\" = \"Snowing without high winds\",\n  \"4\" = \"Fine with high winds\",\n  \"5\" = \"Raining with high winds\",\n  \"6\" = \"Snowing with high winds\",\n  \"7\" = \"Fog or mist\",\n  \"8\" = \"Other\",\n  \"9\" = \"Unknown\"\n)\n\n# Map weather conditions\nroad_casualities$weather_conditions_desc &lt;- as.factor(weather_conditions_map[as.character(road_casualities$weather_conditions)])\n\n# Summarize the data to get accident counts by weather condition\nweather_summary &lt;- road_casualities %&gt;%\n  group_by(weather_conditions_desc) %&gt;%\n  summarise(total_accidents = n(), .groups = \"drop\")\n\n# Add a clean label for the tooltip\nweather_summary$tooltip_label &lt;- paste0(\n  \"Weather: \", weather_summary$weather_conditions_desc, \n  \"&lt;br&gt;Total Accidents: \", weather_summary$total_accidents\n)\n\n# Create a bar plot with dynamic red shading\nweather_plot &lt;- ggplot(weather_summary, aes(\n  x = reorder(weather_conditions_desc, -total_accidents), \n  y = total_accidents, \n  fill = total_accidents, \n  text = tooltip_label  # Use clean tooltip labels\n)) +\n  geom_bar(stat = \"identity\", color = \"black\", alpha = 0.8) +\n  scale_fill_gradient(low = \"#FFC1C1\", high = \"#8B0000\", name = \"Total Accidents\") +\n  labs(\n    title = \"Accidents by Weather Conditions\",\n    x = \"Weather Conditions\",\n    y = \"Number of Accidents\"\n  ) \n\n# Make it interactive and use the clean tooltip\ninteractive_weather_plot &lt;- ggplotly(weather_plot, tooltip = \"text\")\ninteractive_weather_plot\n\n\n\n\n\n\n# Load the dataset\nroad_casualities &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\n\n# Define STATS19 road surface condition mapping\nroad_conditions_map &lt;- c(\n  \"1\" = \"Dry\",\n  \"2\" = \"Wet or damp\",\n  \"3\" = \"Snow\",\n  \"4\" = \"Frost or ice\",\n  \"5\" = \"Flood\",\n  \"6\" = \"Oil or diesel\",\n  \"7\" = \"Mud\",\n  \"8\" = \"Other\",\n  \"9\" = \"Unknown\"\n)\n\n# Step 1: Filter valid road_surface_conditions values (1 to 9)\nroad_casualities &lt;- road_casualities %&gt;%\n  filter(road_surface_conditions %in% c(1:9))\n\n# Step 2: Map road conditions to descriptions\nroad_casualities$road_conditions_desc &lt;- as.factor(\n  road_conditions_map[as.character(road_casualities$road_surface_conditions)]\n)\n\n# Debugging: Check unique values in road_conditions_desc\nprint(\"Mapped descriptions:\")\n\n[1] \"Mapped descriptions:\"\n\nprint(unique(road_casualities$road_conditions_desc))\n\n[1] Wet or damp  Dry          Unknown      Frost or ice Snow        \n[6] Flood       \nLevels: Dry Flood Frost or ice Snow Unknown Wet or damp\n\n# Step 3: Aggregate data by road surface condition\nroad_summary &lt;- road_casualities %&gt;%\n  group_by(road_conditions_desc) %&gt;%\n  summarise(total_accidents = n()) %&gt;%\n  arrange(desc(total_accidents))\n\n# Debugging: Check aggregated data\nprint(\"Aggregated road surface condition data:\")\n\n[1] \"Aggregated road surface condition data:\"\n\nprint(road_summary)\n\n# A tibble: 6 × 2\n  road_conditions_desc total_accidents\n  &lt;fct&gt;                          &lt;int&gt;\n1 Dry                            72752\n2 Wet or damp                    26944\n3 Unknown                         1617\n4 Frost or ice                    1461\n5 Snow                             241\n6 Flood                            179\n\n# Step 4: Create the bar plot\nroad_plot &lt;- ggplot(road_summary, aes(\n  x = reorder(road_conditions_desc, -total_accidents), \n  y = total_accidents, \n  fill = road_conditions_desc, \n  text = paste0(road_conditions_desc, \": \", total_accidents, \" accidents\")\n)) +\n  geom_bar(stat = \"identity\", alpha = 0.8, color = \"black\") +\n  labs(\n    title = \"Distribution of Accidents by Road Surface Conditions (STATS19)\",\n    x = \"Road Surface Conditions\",\n    y = \"Number of Accidents\"\n  ) +\n  scale_fill_manual(values = c(\n    \"Dry\" = \"red\",\n    \"Wet or damp\" = \"#73a2c6\",\n    \"Snow\" = \"#1b9e77\",\n    \"Frost or ice\" = \"#d95f02\",\n    \"Flood\" = \"#7570b3\",\n    \"Oil or diesel\" = \"#e7298a\",\n    \"Mud\" = \"#66a61e\",\n    \"Other\" = \"#e6ab02\",\n    \"Unknown\" = \"#a6761d\"\n  )) \n\n# Convert the plot to an interactive plot\ninteractive_road_plot &lt;- ggplotly(road_plot, tooltip = \"text\")\n\n# Display the interactive plot\ninteractive_road_plot\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(caret)\n# Load necessary libraries\nlibrary(nnet)  # For multinomial logistic regression\nlibrary(caret) # For data partitioning\n\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n\n# Select relevant columns and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, c(\"accident_severity\", \"weather_conditions\")])\n\n# Convert columns to factors\ndata_cleaned$accident_severity &lt;- as.factor(data_cleaned$accident_severity)\ndata_cleaned$weather_conditions &lt;- as.numeric(data_cleaned$weather_conditions)\n\n# Split the dataset into training and testing sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(data_cleaned$accident_severity, p = 0.8, list = FALSE)\ntrain_data &lt;- data_cleaned[train_index, ]\ntest_data &lt;- data_cleaned[-train_index, ]\n\n# Train multinomial logistic regression model\nmultinom_model &lt;- multinom(accident_severity ~ weather_conditions, data = train_data)\n\n# weights:  9 (4 variable)\ninitial  value 91633.053773 \niter  10 value 50406.679444\nfinal  value 50392.493956 \nconverged\n\n# Summarize the model\nsummary(multinom_model)\n\nCall:\nmultinom(formula = accident_severity ~ weather_conditions, data = train_data)\n\nCoefficients:\n  (Intercept) weather_conditions\n2     2.64414         0.06126746\n3     3.76240         0.12285242\n\nStd. Errors:\n  (Intercept) weather_conditions\n2  0.04240724         0.02156981\n3  0.04154713         0.02120177\n\nResidual Deviance: 100785 \nAIC: 100793 \n\n# Make predictions\npredictions &lt;- predict(multinom_model, newdata = test_data)\n\n# Evaluate the model\nconfusion_matrix &lt;- table(test_data$accident_severity, predictions)\nprint(\"Confusion Matrix:\")\n\n[1] \"Confusion Matrix:\"\n\nprint(confusion_matrix)\n\n   predictions\n        1     2     3\n  1     0     0   304\n  2     0     0  4687\n  3     0     0 15859\n\n# Calculate accuracy\naccuracy &lt;- sum(diag(confusion_matrix)) / sum(confusion_matrix)\nprint(paste(\"Accuracy:\", round(accuracy, 4)))\n\n[1] \"Accuracy: 0.7606\"\n\n\n\n# Load necessary libraries\nlibrary(nnet)  # For multinomial logistic regression\nlibrary(caret) # For data partitioning\nlibrary(pROC)  # For AUC and ROC curves\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Select relevant columns and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, c(\"accident_severity\", \"weather_conditions\")])\n\n# Convert columns to factors and numeric types\ndata_cleaned$accident_severity &lt;- as.factor(data_cleaned$accident_severity)\ndata_cleaned$weather_conditions &lt;- as.numeric(data_cleaned$weather_conditions)\n\n# Split the dataset into training and testing sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(data_cleaned$accident_severity, p = 0.8, list = FALSE)\ntrain_data &lt;- data_cleaned[train_index, ]\ntest_data &lt;- data_cleaned[-train_index, ]\n\n# Train multinomial logistic regression model\nmultinom_model &lt;- multinom(accident_severity ~ weather_conditions, data = train_data)\n\n# weights:  9 (4 variable)\ninitial  value 91633.053773 \niter  10 value 50406.679444\nfinal  value 50392.493956 \nconverged\n\n# Predict probabilities for the test data\nprobabilities &lt;- predict(multinom_model, newdata = test_data, type = \"probs\")\n\n# Create one-vs-all ROC curves for each class\nroc_curve_1 &lt;- roc(as.numeric(test_data$accident_severity == 1), probabilities[, 1], plot = TRUE, col = \"red\", main = \"ROC Curve for Multinomial Logistic Regression\")\nroc_curve_2 &lt;- roc(as.numeric(test_data$accident_severity == 2), probabilities[, 2], plot = TRUE, col = \"blue\", add = TRUE)\nroc_curve_3 &lt;- roc(as.numeric(test_data$accident_severity == 3), probabilities[, 3], plot = TRUE, col = \"green\", add = TRUE)\n\n# Add legend\nlegend(\"bottomright\", legend = c(\"Class 1 (Fatal)\", \"Class 2 (Serious)\", \"Class 3 (Slight)\"),\n       col = c(\"red\", \"blue\", \"green\"), lwd = 2)\n\n\n\n\n\n\n\n# Compute AUC for each class\nauc_1 &lt;- auc(roc_curve_1)\nauc_2 &lt;- auc(roc_curve_2)\nauc_3 &lt;- auc(roc_curve_3)\n\n\n# Load necessary libraries\n# Load necessary libraries\nlibrary(randomForest)\nlibrary(pROC)\nlibrary(caret)\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Select relevant features and target variable\nfeatures &lt;- c(\"weather_conditions\", \"number_of_vehicles\", \"road_surface_conditions\", \"urban_or_rural_area\")\ntarget &lt;- \"accident_severity\"\n\n# Filter data to include only relevant columns and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, c(features, target)])\n\n# Convert target variable to factor for classification\ndata_cleaned$accident_severity &lt;- as.factor(data_cleaned$accident_severity)\n\n# Split data into training and testing sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(data_cleaned$accident_severity, p = 0.8, list = FALSE)\ntrain_data &lt;- data_cleaned[train_index, ]\ntest_data &lt;- data_cleaned[-train_index, ]\n\n# Train a Random Forest model\nrf_model &lt;- randomForest(\n  accident_severity ~ .,\n  data = train_data,\n  ntree = 100,\n  importance = TRUE\n)\n\n# Print model summary\nprint(rf_model)\n\n\nCall:\n randomForest(formula = accident_severity ~ ., data = train_data,      ntree = 100, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 23.95%\nConfusion matrix:\n  1  2     3  class.error\n1 0  0  1218 1.0000000000\n2 0  6 18745 0.9996800171\n3 0 15 63424 0.0002364476\n\n# Predict probabilities on the test set\nrf_probabilities &lt;- predict(rf_model, newdata = test_data, type = \"prob\")\n\n# Draw AUC Curve for each class using one-vs-all approach\nauc_values &lt;- list()\nfor (i in 1:ncol(rf_probabilities)) {\n  class_label &lt;- colnames(rf_probabilities)[i]\n  roc_curve &lt;- roc(as.numeric(test_data$accident_severity == class_label), \n                   rf_probabilities[, i], \n                   plot = TRUE, \n                   main = paste(\"ROC Curve for Class\", class_label), \n                   col = i)\n  auc_values[[class_label]] &lt;- auc(roc_curve)\n  print(paste(\"AUC for Class\", class_label, \":\", round(auc(roc_curve), 4)))\n}\n\n\n\n\n\n\n\n\n[1] \"AUC for Class 1 : 0.4996\"\n\n\n\n\n\n\n\n\n\n[1] \"AUC for Class 2 : 0.5012\"\n\n\n\n\n\n\n\n\n\n[1] \"AUC for Class 3 : 0.5012\"\n\n# Visualize feature importance\nvarImpPlot(rf_model)"
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "The data and insights used in this project are sourced from the following:\nData CSV: - casualty_statistics.csv\nClick the links above to download the CSV files used for the analysis.\n\nReferences\n[1] Department for Transport, “Road Safety Data,” data.gov.uk, [Online]. Available: https://www.data.gov.uk/dataset/cb7ae6f0-4be6-49359277-47e5ce24a11f/road-safety-data. [Accessed: Dec. 11, 2024].\n[2]“Canvas Login | Instructure,” Gmu.edu, 2024. https://canvas.gmu.edu/courses/25180/files/7132982?module_item_id=2696644&fd_cookie_set=1 (accessed Dec. 11, 2024).\n‌[3]“Creating websites with Quarto and GitHub,” YouTube. http://www.youtube.com/playlist?list=PLkrJrLs7xfbXcEKhTCKRSr2VXH4yiBeXo (accessed Dec. 11, 2024).\n‌"
  },
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Paving the Way to Safety: A Statistical Exploration of UK Road Accidents",
    "section": "",
    "text": "Insightfully crafted by:\nVENKATA LAKSHMI PARIMALA PASUPULETI\nANNAM KAVYA\nKIRAN DABBIRIL\nRoad accidents remain a significant public health concern, impacting thousands of lives every year. By analyzing road accidents and safety statistics, we can identify trends, evaluate safety measures, and propose data-driven solutions to improve transportation systems. This project focuses on UK road accident and safety statistics, leveraging data from the government’s comprehensive guidance."
  },
  {
    "objectID": "home.html#understanding-global-suicide-rates-1950-2022",
    "href": "home.html#understanding-global-suicide-rates-1950-2022",
    "title": "Welcome to the Global Suicide Data Visualization!",
    "section": "",
    "text": "Suicide, a complex public health issue, is responsible for approximately 828,000 deaths globally each year. Understanding the factors contributing to suicide and effectively visualizing this data can help reduce stigma and foster preventive measures. This project aims to provide a clearer understanding of suicide rates worldwide from 1950 to 2022."
  },
  {
    "objectID": "home.html#key-insights",
    "href": "home.html#key-insights",
    "title": "Welcome to the Global Suicide Data Visualization!",
    "section": "Key Insights:",
    "text": "Key Insights:"
  },
  {
    "objectID": "home.html#global-trends",
    "href": "home.html#global-trends",
    "title": "Welcome to the Global Suicide Data Visualization!",
    "section": "Global Trends",
    "text": "Global Trends\nThe project analyzes variations in suicide rates across different countries, highlighting the alarming statistics in nations such as Lithuania and South Korea."
  },
  {
    "objectID": "home.html#demographic-focus",
    "href": "home.html#demographic-focus",
    "title": "Welcome to the Global Suicide Data Visualization!",
    "section": "Demographic Focus",
    "text": "Demographic Focus\nWe break down data by age groups to identify trends, particularly concerning the vulnerability of younger populations."
  },
  {
    "objectID": "home.html#visual-improvements",
    "href": "home.html#visual-improvements",
    "title": "Welcome to the Global Suicide Data Visualization!",
    "section": "Visual Improvements",
    "text": "Visual Improvements\nPrevious visualizations faced issues like clutter and poor categorization. Our re-visualization employs world maps and interactive graphs to enhance clarity and engagement."
  },
  {
    "objectID": "home.html#visualizations-include",
    "href": "home.html#visualizations-include",
    "title": "Welcome to the Global Suicide Data Visualization!",
    "section": "Visualizations Include",
    "text": "Visualizations Include\n\nAverage Suicide Rates by Country (1950-2022): An interactive world map displaying the average suicide rates per 100,000 people, allowing for a deeper exploration of regional data.\nPeak Suicide Rates (1982): Visualization of the highest average suicide rates by country during this pivotal year.\nYearly Trends (1950-2022): A frequency polygon graph showcasing the global trend in suicide rates over time.\nTop Countries and Years: Bar graphs detailing the top five countries with the highest suicide rates and their corresponding peak years."
  },
  {
    "objectID": "home.html#objective",
    "href": "home.html#objective",
    "title": "Welcome to the Global Suicide Data Visualization!",
    "section": "Objective",
    "text": "Objective\nThis project not only aims to present statistical data effectively but also to foster discussions on suicide prevention and the importance of mental health awareness."
  },
  {
    "objectID": "home.html#more-information",
    "href": "home.html#more-information",
    "title": "Welcome to the Global Suicide Data Visualization!",
    "section": "More Information",
    "text": "More Information\nYou can find more information about the project visualization, the code used, and their references on the following pages:\n\nProject Summary\nProject Source Code\nReferences"
  },
  {
    "objectID": "about.html#software-engineer-october-2018---august-2024",
    "href": "about.html#software-engineer-october-2018---august-2024",
    "title": "About Us",
    "section": "Software Engineer (October 2018 - August 2024)",
    "text": "Software Engineer (October 2018 - August 2024)\nIn this role, I focused on developing scalable solutions within the property management software sector. My key responsibilities included:\n\nDatabase Architecture and Optimization: Collaborated with cross-functional teams to build robust database structures and optimized SQL queries to enhance overall system efficiency.\nProduct Development: Created and deployed microservicable product which includes the improved system performance.\nClient Support: Delivering optimization solutions to enhance product performance and address client requirements.\nDatabase administration: Acted as a virtual database administrator and helped the team with various database solutions.\nProject Leading: Led various projects, contributing to project management and project support."
  },
  {
    "objectID": "about.html#associate-software-engineer-july-2017---september-2018",
    "href": "about.html#associate-software-engineer-july-2017---september-2018",
    "title": "About Us",
    "section": "Associate Software Engineer (July 2017 - September 2018)",
    "text": "Associate Software Engineer (July 2017 - September 2018)\nAs an Associate Software Engineer, I gained foundational skills in web development and database management. My contributions included:\n\nAgile Practices: Learned agile framework to deploy efficient projects, learned sprint deployment and retrospectives to ensure efficient project progress.\nTechnical Support: Provided troubleshooting assistance to resolve client issues, enhancing user experience.\nMentorship: Collaborated with senior engineers, learning advanced database management techniques and best practices."
  },
  {
    "objectID": "about.html#trainee-january-2017---june-2017",
    "href": "about.html#trainee-january-2017---june-2017",
    "title": "About Us",
    "section": "Trainee (January 2017 - June 2017)",
    "text": "Trainee (January 2017 - June 2017)\nIn this initial role, I focused on acquiring essential skills in software development and database management. My responsibilities included:\n\nTraining and Development: Engaged in training sessions with experienced engineers, laying the groundwork for my subsequent roles.\nProject Assistance: Assisted in various projects, contributing to code development and testing efforts.\n\n\nKey Projects\n\nMicroservices Architecture Development: Spearheaded a project that improved scalability by 50% and reduced deployment times.\nAWS Cloud Migration: Led the migration of 40% of client documents from on-premises servers to AWS S3, solving critical storage challenges.\nAutomated Data Import into the system: Redesigned a data module using automation with Python and OpenAI, to import 1,50,000 records every year resulting in more efficient data handling and improved eligibility calculations for affordable housing product.\nSensitive Data Encryption: Worked on encrypting sensitive resident data across the entire database to ensure compliance with security standards.\nDocument Creation automation: Developed more than 56 state-specific certification documents for affordable housing using custom scripts and database integration and automated the process of filling the certifications for every resident on certification process using object-oriented PHP, PostgreSQL, and Adobe techniques.\n\n\n\nTechnical Skills\n\nDatabases: PostgreSQL, MySQL\n\nBackend Development: PHP, Python\n\nFrontend Technologies: AJAX, HTML, JavaScript, jQuery\n\nCloud Technologies: AWS, Docker, Kubernetes\n\nFrameworks: MVC, Singleton, Fusebox\n\nAgile Tools: Jira, ClickUp\n\nVersion Control: GIT, SVN\n\n\n\nEducation\n\nMaster’s in Data Analytics Engineering\nGeorge Mason University, Virginia, USA (2024 - 2026)\nBachelor of Technology in Computer Science & Technology\nSree Venkateshwara College of Engineering, Nellore, India (2013 - 2017)"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact Information",
    "section": "",
    "text": "If you would like to get in touch, feel free to reach out through the following:\n\nVenkata Lakshmi Parimala Pasupuleti\n\nEmail: venkatalakshmiparimala@gmail.com\nLinkedIn: Venkata Lakshmi Parimala Pasupuleti\n\n\n\nKiran Dabbiril\n\nEmail: dabbiru.kiran03@gmail.com\nPhone: +1 571-663-5603\n\nAnnam Kavya\n\nEmail: Annamkavya@gmail.com\n\nWe look forward to hearing from you!"
  },
  {
    "objectID": "project_source_code.html#only-logistic-regression-with-2-variables",
    "href": "project_source_code.html#only-logistic-regression-with-2-variables",
    "title": "Project Source Code",
    "section": "only Logistic regression with 2 variables",
    "text": "only Logistic regression with 2 variables\n\n# Load necessary libraries\nlibrary(nnet)    # For logistic regression\nlibrary(pROC)    # For ROC curve and AUC\nlibrary(caret)   # For data partitioning\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Select relevant columns and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, c(\"accident_severity\", \"urban_or_rural_area\", \"weather_conditions\")])\n\n# Create a binary target variable (1 for Slight, 0 for Fatal and Serious)\ndata_cleaned$binary_severity &lt;- ifelse(data_cleaned$accident_severity == 3, 1, 0)\n\n# Convert necessary columns to numeric/factor\ndata_cleaned$binary_severity &lt;- as.factor(data_cleaned$binary_severity)\ndata_cleaned$urban_or_rural_area &lt;- as.numeric(data_cleaned$urban_or_rural_area)\ndata_cleaned$weather_conditions &lt;- as.numeric(data_cleaned$weather_conditions)\n\n# Split the dataset into training and testing sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(data_cleaned$binary_severity, p = 0.8, list = FALSE)\ntrain_data &lt;- data_cleaned[train_index, ]\ntest_data &lt;- data_cleaned[-train_index, ]\n\n# Train logistic regression model\nlogistic_model &lt;- glm(binary_severity ~ urban_or_rural_area + weather_conditions, \n                      data = train_data, \n                      family = binomial)\nprint(logistic_model)\n\n\nCall:  glm(formula = binary_severity ~ urban_or_rural_area + weather_conditions, \n    family = binomial, data = train_data)\n\nCoefficients:\n        (Intercept)  urban_or_rural_area   weather_conditions  \n            1.60582             -0.40781              0.06153  \n\nDegrees of Freedom: 83406 Total (i.e. Null);  83404 Residual\nNull Deviance:      91810 \nResidual Deviance: 91030    AIC: 91040\n\n# Predict probabilities for the test data\nprobabilities &lt;- predict(logistic_model, newdata = test_data, type = \"response\")\n\n# Create the ROC curve\nroc_curve &lt;- roc(test_data$binary_severity, probabilities, plot = TRUE, col = \"blue\", \n                 main = \"ROC Curve for Binary Classification\")\n\n\n\n\n\n\n\n# Compute and print AUC\nauc_value &lt;- auc(roc_curve)\nprint(paste(\"AUC Value:\", round(auc_value, 4)))\n\n[1] \"AUC Value: 0.5498\"\n\n# Add thresholds to the ROC plot\nplot(roc_curve, print.auc = TRUE, col = \"blue\")"
  },
  {
    "objectID": "project_source_code.html#logistic-regression-lasso-wirth-2-combination-variables",
    "href": "project_source_code.html#logistic-regression-lasso-wirth-2-combination-variables",
    "title": "Project Source Code",
    "section": "Logistic regression Lasso wirth 2 combination variables",
    "text": "Logistic regression Lasso wirth 2 combination variables\n\nlibrary(glmnet)\n\n# Prepare data for glmnet\nX &lt;- as.matrix(train_data[, c(\"urban_or_rural_area\", \"weather_conditions\")])\ny &lt;- as.numeric(train_data$binary_severity) - 1\n\n# Train Lasso logistic regression\nlasso_model &lt;- cv.glmnet(X, y, family = \"binomial\", alpha = 1)\n\n# Predict probabilities for test data\ntest_X &lt;- as.matrix(test_data[, c(\"urban_or_rural_area\", \"weather_conditions\")])\nlasso_probabilities &lt;- predict(lasso_model, newx = test_X, s = \"lambda.min\", type = \"response\")\n\n# Evaluate using ROC and AUC\nlasso_roc &lt;- roc(as.numeric(test_data$binary_severity), lasso_probabilities, plot = TRUE, col = \"blue\",\n                 main = \"ROC Curve for Lasso Logistic Regression\")\n\n\n\n\n\n\n\nlasso_auc &lt;- auc(lasso_roc)\nprint(paste(\"Lasso Logistic Regression AUC:\", round(lasso_auc, 4)))\n\n[1] \"Lasso Logistic Regression AUC: 0.5514\""
  },
  {
    "objectID": "project_source_code.html#logistic-regression-lasso-wirth-more-variables",
    "href": "project_source_code.html#logistic-regression-lasso-wirth-more-variables",
    "title": "Project Source Code",
    "section": "Logistic regression Lasso wirth more variables",
    "text": "Logistic regression Lasso wirth more variables\n\n# Load necessary libraries\nlibrary(glmnet)\nlibrary(pROC)\n\n# Load the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Define selected features and target variable\nselected_features &lt;- c(\"accident_severity\", \"number_of_vehicles\", \"road_surface_conditions\", \n                       \"weather_conditions\", \"urban_or_rural_area\", \"special_conditions_at_site\", \n                       \"day_of_week\", \"time\", \"junction_detail\", \"speed_limit\")\n\n# Check if selected features exist in the dataset\nif (!all(selected_features %in% colnames(data))) {\n  missing_features &lt;- selected_features[!selected_features %in% colnames(data)]\n  stop(paste(\"The following features are missing from the dataset:\", paste(missing_features, collapse = \", \")))\n}\n\n# Filter the dataset for selected features and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, selected_features])\n\n# Create a binary target variable for classification\n# Severe (1) = accident_severity 1 or 2, Slight (0) = accident_severity 3\ndata_cleaned$binary_severity &lt;- ifelse(data_cleaned$accident_severity == 3, 0, 1)\n\n# Feature engineering: Convert 'time' into 'time_of_day' (e.g., Morning, Afternoon, Night)\ndata_cleaned$time &lt;- as.numeric(sub(\"^(\\\\d{2}):.*$\", \"\\\\1\", data_cleaned$time)) # Extract hour\ndata_cleaned$time_of_day &lt;- cut(data_cleaned$time, \n                                breaks = c(-1, 6, 12, 18, 24), \n                                labels = c(\"Night\", \"Morning\", \"Afternoon\", \"Evening\"))\n\n# Drop unnecessary columns (original 'time' and 'accident_severity')\ndata_cleaned &lt;- data_cleaned[, !(names(data_cleaned) %in% c(\"time\", \"accident_severity\"))]\n\n# Convert categorical variables into factors\ncategorical_vars &lt;- c(\"road_surface_conditions\", \"weather_conditions\", \"urban_or_rural_area\", \n                      \"special_conditions_at_site\", \"day_of_week\", \"time_of_day\", \"junction_detail\")\ndata_cleaned[categorical_vars] &lt;- lapply(data_cleaned[categorical_vars], as.factor)\n\n# Prepare data for glmnet\nX &lt;- model.matrix(binary_severity ~ ., data_cleaned)[, -1] # Remove intercept\ny &lt;- data_cleaned$binary_severity\n\n# Split data into training and testing sets\nset.seed(42)\ntrain_index &lt;- sample(1:nrow(X), size = 0.8 * nrow(X))\ntrain_X &lt;- X[train_index, ]\ntrain_y &lt;- y[train_index]\ntest_X &lt;- X[-train_index, ]\ntest_y &lt;- y[-train_index]\n\n# Train Lasso Logistic Regression model\nlasso_model &lt;- cv.glmnet(train_X, train_y, family = \"binomial\", alpha = 1)\n\n# Predict probabilities for test data\nlasso_probabilities &lt;- predict(lasso_model, newx = test_X, s = \"lambda.min\", type = \"response\")\n\n# Evaluate using ROC and AUC\nlasso_roc &lt;- roc(test_y, lasso_probabilities, plot = TRUE, col = \"blue\",\n                 main = \"ROC Curve for Lasso Logistic Regression\")\n\n\n\n\n\n\n\nlasso_auc &lt;- auc(lasso_roc)\nprint(paste(\"Lasso Logistic Regression AUC:\", round(lasso_auc, 4)))\n\n[1] \"Lasso Logistic Regression AUC: 0.612\"\n\n# Feature importance (coefficients)\ncoefficients &lt;- coef(lasso_model, s = \"lambda.min\")\nprint(\"Selected Features and Coefficients:\")\n\n[1] \"Selected Features and Coefficients:\"\n\nprint(coefficients)\n\n48 x 1 sparse Matrix of class \"dgCMatrix\"\n                                      s1\n(Intercept)                 -1.159394968\nnumber_of_vehicles          -0.256295633\nroad_surface_conditions1     0.937290733\nroad_surface_conditions2     0.925376470\nroad_surface_conditions3     0.958261001\nroad_surface_conditions4     0.708252313\nroad_surface_conditions5     0.944823220\nroad_surface_conditions9     0.445159967\nweather_conditions2         -0.069904189\nweather_conditions3         -0.117426522\nweather_conditions4          0.155343359\nweather_conditions5         -0.040067944\nweather_conditions6         -0.075081224\nweather_conditions7         -0.284458507\nweather_conditions8         -0.169295042\nweather_conditions9         -0.383972781\nurban_or_rural_area1        -0.112744643\nurban_or_rural_area2         0.086384660\nurban_or_rural_area3         0.973339463\nspecial_conditions_at_site0 -0.306314034\nspecial_conditions_at_site1 -0.507191456\nspecial_conditions_at_site2  0.314806500\nspecial_conditions_at_site3 -0.115356865\nspecial_conditions_at_site4 -0.412954815\nspecial_conditions_at_site5 -0.116574223\nspecial_conditions_at_site6 -0.353961785\nspecial_conditions_at_site7 -0.336745861\nspecial_conditions_at_site9 -1.031307541\nday_of_week2                -0.136292827\nday_of_week3                -0.175948703\nday_of_week4                -0.120039691\nday_of_week5                -0.124894449\nday_of_week6                -0.104736531\nday_of_week7                -0.002913686\njunction_detail0             0.091600324\njunction_detail1            -0.453624650\njunction_detail2            -0.188097911\njunction_detail3             .          \njunction_detail5            -0.201307564\njunction_detail6             0.008137821\njunction_detail7             .          \njunction_detail8             .          \njunction_detail9             0.001650131\njunction_detail99           -1.819330521\nspeed_limit                  0.007150706\ntime_of_dayMorning          -0.318457391\ntime_of_dayAfternoon        -0.265087170\ntime_of_dayEvening          -0.113424581"
  },
  {
    "objectID": "CrashClarity.html#loading-libraries",
    "href": "CrashClarity.html#loading-libraries",
    "title": "CRASH CLARITY",
    "section": "Loading Libraries:",
    "text": "Loading Libraries:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] \"Mapped descriptions:\"\n\n\n[1] Wet or damp  Dry          Unknown      Frost or ice Snow        \n[6] Flood       \nLevels: Dry Flood Frost or ice Snow Unknown Wet or damp\n\n\n[1] \"Aggregated road surface condition data:\"\n\n\n# A tibble: 6 × 2\n  road_conditions_desc total_accidents\n  &lt;fct&gt;                          &lt;int&gt;\n1 Dry                            72752\n2 Wet or damp                    26944\n3 Unknown                         1617\n4 Frost or ice                    1461\n5 Snow                             241\n6 Flood                            179\n\n\n\n\n\n\n\n\n# weights:  9 (4 variable)\ninitial  value 91633.053773 \niter  10 value 50406.679444\nfinal  value 50392.493956 \nconverged\n\n\nCall:\nmultinom(formula = accident_severity ~ weather_conditions, data = train_data)\n\nCoefficients:\n  (Intercept) weather_conditions\n2     2.64414         0.06126746\n3     3.76240         0.12285242\n\nStd. Errors:\n  (Intercept) weather_conditions\n2  0.04240724         0.02156981\n3  0.04154713         0.02120177\n\nResidual Deviance: 100785 \nAIC: 100793 \n\n\n[1] \"Confusion Matrix:\"\n\n\n   predictions\n        1     2     3\n  1     0     0   304\n  2     0     0  4687\n  3     0     0 15859\n\n\n[1] \"Accuracy: 0.7606\"\n\n\n\n\n# weights:  9 (4 variable)\ninitial  value 91633.053773 \niter  10 value 50406.679444\nfinal  value 50392.493956 \nconverged\n\n\n\n\n\n\n\n\n\n\n\n\nCall:\n randomForest(formula = accident_severity ~ ., data = train_data,      ntree = 100, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 23.95%\nConfusion matrix:\n  1  2     3  class.error\n1 0  0  1218 1.0000000000\n2 0  6 18745 0.9996800171\n3 0 15 63424 0.0002364476\n\n\n\n\n\n\n\n\n\n[1] \"AUC for Class 1 : 0.4996\"\n\n\n\n\n\n\n\n\n\n[1] \"AUC for Class 2 : 0.5012\"\n\n\n\n\n\n\n\n\n\n[1] \"AUC for Class 3 : 0.5012\""
  },
  {
    "objectID": "CrashClarity.html#only-logistic-regression-with-2-variables",
    "href": "CrashClarity.html#only-logistic-regression-with-2-variables",
    "title": "CRASH CLARITY",
    "section": "only Logistic regression with 2 variables",
    "text": "only Logistic regression with 2 variables\n\nLogistic Regression for Binary Accident Severity Classification\nA logistic regression model was employed to classify accident severity into binary categories: “Slight” (1) and “Fatal/Serious” (0), using features such as urban or rural area and weather conditions. The model showed a modest reduction in residual deviance (from 91810 to 91030), with an AIC of 91040, suggesting limited improvement over the null model.\nThe ROC curve yielded an AUC value of 0.5498, indicating the model’s predictive performance is slightly better than random chance. The negative coefficient for “urban_or_rural_area” suggests that accidents in urban areas are more likely to be classified as “Slight,” while the positive coefficient for “weather_conditions” implies a weak association with increased severity. Overall, the model demonstrates minimal predictive capability and requires additional features or refinement to achieve better classification accuracy and practical applicability.\n\n\n\nCall:  glm(formula = binary_severity ~ urban_or_rural_area + weather_conditions, \n    family = binomial, data = train_data)\n\nCoefficients:\n        (Intercept)  urban_or_rural_area   weather_conditions  \n            1.60582             -0.40781              0.06153  \n\nDegrees of Freedom: 83406 Total (i.e. Null);  83404 Residual\nNull Deviance:      91810 \nResidual Deviance: 91030    AIC: 91040\n\n\n\n\n\n\n\n\n\n[1] \"AUC Value: 0.5498\""
  },
  {
    "objectID": "CrashClarity.html#logistic-regression-lasso-wirth-2-combination-variables",
    "href": "CrashClarity.html#logistic-regression-lasso-wirth-2-combination-variables",
    "title": "CRASH CLARITY",
    "section": "Logistic regression Lasso wirth 2 combination variables",
    "text": "Logistic regression Lasso wirth 2 combination variables\nLasso Logistic Regression for Binary Classification of Accident Severity\nA Lasso logistic regression model was applied to classify accident severity into binary categories (“Slight” vs. “Fatal/Serious”) using features such as urban or rural area and weather conditions. The model employed cross-validation to identify the optimal regularization parameter (lambda.min), ensuring reduced overfitting and improved generalizability.\nThe ROC curve yielded an AUC of 0.5514, indicating a marginally better performance than random guessing. The close proximity of the ROC curve to the diagonal reference line suggests limited predictive power. While the model effectively reduces feature complexity, the low AUC highlights the need for additional predictive variables or refined feature engineering to improve classification accuracy and ensure practical applicability.\n\n\n\n\n\n\n\n\n\n[1] \"Lasso Logistic Regression AUC: 0.5514\""
  },
  {
    "objectID": "CrashClarity.html#logistic-regression-lasso-wirth-more-variables",
    "href": "CrashClarity.html#logistic-regression-lasso-wirth-more-variables",
    "title": "CRASH CLARITY",
    "section": "Logistic regression Lasso wirth more variables",
    "text": "Logistic regression Lasso wirth more variables\nLasso Logistic Regression for Predicting Accident Severity\nA Lasso logistic regression model was implemented to classify accident severity into binary categories: “Severe” (1) and “Slight” (0). The model utilized key features such as road surface conditions, weather conditions, urban or rural area, and time of day. Cross-validation identified the optimal regularization parameter (lambda.min), ensuring feature selection and preventing over fitting.\nThe model achieved an AUC of 0.612, which is the highest among all models evaluated, as visualized through the ROC curve. This indicates improved predictive performance and better discriminatory power compared to earlier approaches. Feature coefficients highlighted the importance of road surface conditions and urban/rural areas as significant predictors. While the model demonstrates improved performance, further enhancements could refine its applicability for real-world scenarios.\n\n\n\n\n\n\n\n\n\n[1] \"Lasso Logistic Regression AUC: 0.612\"\n\n\n[1] \"Selected Features and Coefficients:\"\n\n\n48 x 1 sparse Matrix of class \"dgCMatrix\"\n                                      s1\n(Intercept)                 -1.159394968\nnumber_of_vehicles          -0.256295633\nroad_surface_conditions1     0.937290733\nroad_surface_conditions2     0.925376470\nroad_surface_conditions3     0.958261001\nroad_surface_conditions4     0.708252313\nroad_surface_conditions5     0.944823220\nroad_surface_conditions9     0.445159967\nweather_conditions2         -0.069904189\nweather_conditions3         -0.117426522\nweather_conditions4          0.155343359\nweather_conditions5         -0.040067944\nweather_conditions6         -0.075081224\nweather_conditions7         -0.284458507\nweather_conditions8         -0.169295042\nweather_conditions9         -0.383972781\nurban_or_rural_area1        -0.112744643\nurban_or_rural_area2         0.086384660\nurban_or_rural_area3         0.973339463\nspecial_conditions_at_site0 -0.306314034\nspecial_conditions_at_site1 -0.507191456\nspecial_conditions_at_site2  0.314806500\nspecial_conditions_at_site3 -0.115356865\nspecial_conditions_at_site4 -0.412954815\nspecial_conditions_at_site5 -0.116574223\nspecial_conditions_at_site6 -0.353961785\nspecial_conditions_at_site7 -0.336745861\nspecial_conditions_at_site9 -1.031307541\nday_of_week2                -0.136292827\nday_of_week3                -0.175948703\nday_of_week4                -0.120039691\nday_of_week5                -0.124894449\nday_of_week6                -0.104736531\nday_of_week7                -0.002913686\njunction_detail0             0.091600324\njunction_detail1            -0.453624650\njunction_detail2            -0.188097911\njunction_detail3             .          \njunction_detail5            -0.201307564\njunction_detail6             0.008137821\njunction_detail7             .          \njunction_detail8             .          \njunction_detail9             0.001650131\njunction_detail99           -1.819330521\nspeed_limit                  0.007150706\ntime_of_dayMorning          -0.318457391\ntime_of_dayAfternoon        -0.265087170\ntime_of_dayEvening          -0.113424581"
  },
  {
    "objectID": "home.html#understanding-road-safety-statistics-in-the-uk",
    "href": "home.html#understanding-road-safety-statistics-in-the-uk",
    "title": "Paving the Way to Safety: A Statistical Exploration of UK Road Accidents",
    "section": "",
    "text": "Road accidents remain a significant public health concern, impacting thousands of lives every year. By analyzing road accidents and safety statistics, we can identify trends, evaluate safety measures, and propose data-driven solutions to improve transportation systems. This project focuses on UK road accident and safety statistics, leveraging data from the government’s comprehensive guidance.\n\n\n\n\n\n\n\nGlobal Comparisons\nThis project explores how the UK fares in terms of road safety compared to other nations, identifying areas of success and potential improvement.\nDemographic Breakdown\nWe analyze accident data across age groups, vehicle types, and geographical regions to understand the most vulnerable populations and high-risk areas.\nVisual Enhancements\nPrevious visualizations lacked interactivity and coherence. Our redesigned charts and maps aim to improve clarity, accessibility, and user engagement.\n\n\n\n\nRoad Accident Trends (2023): Interactive line graphs showcasing the annual trends in accidents, injuries, and fatalities over time.\nImpact of Light and Weather Conditions on Road Accidents using a heatmap: The heatmap reveals that road accidents are most frequent under “Daylight” and “Fine without high winds” conditions, with notable variations across other light and weather scenarios.\nDistribution of Accident Severity: The histogram highlights that “Mild” accidents occur most frequently, followed by “Significant” and “Life-Threatening” incidents, indicating a skew toward less severe outcomes.\nAccident Trends Across Time Bands: The visualization shows that “Daytime” experiences the highest number of accidents, followed by “Morning Rush Hour” and “Evening Rush Hour,” with significantly fewer incidents during nighttime hours.\nImpact of Weather on Road Accidents: The bar plot highlights that the majority of road accidents occur under “Fine without high winds” conditions, with fewer incidents recorded in adverse weather like fog or snow.\nImpact of Road Surface Conditions on Accidents: The visualization shows that most accidents occur on “Dry” surfaces, followed by “Wet or damp” conditions, while adverse surfaces like “Snow,” “Mud,” and “Flood” have significantly fewer incidents.\n\n\n\n\nThis project aims to utilize statistical data to foster discussions on road safety, evaluate the impact of past measures, and provide actionable insights for future improvements. By presenting this data visually, we hope to engage policymakers, researchers, and the public in creating safer roads for everyone.\n\n\n\nYou can find more information about the research questions, project visualization, the code used, and their references on the following pages:\n\nProject Summary\nProject Source Code\nReferences"
  },
  {
    "objectID": "home.html#understanding-road-safety-statistics",
    "href": "home.html#understanding-road-safety-statistics",
    "title": "Paving the Way to Safety: A Statistical Exploration of UK Road Accidents",
    "section": "Understanding Road Safety Statistics",
    "text": "Understanding Road Safety Statistics"
  },
  {
    "objectID": "CrashClarity.html",
    "href": "CrashClarity.html",
    "title": "CRASH CLARITY",
    "section": "",
    "text": "ABSTRACT\nRoad accidents and safety remain critical public health concerns worldwide, with significant societal, economic, and emotional impacts. In the United Kingdom, the government provides comprehensive data on road accidents through its Road Accident and Safety Statistics guidance. This academic project leverages these statistics to analyze and interpret the trends, patterns, and contributing factors associated with road accidents in the UK.\nThe study explores key variables such as accident severity, weather and road conditions, time of day, and demographic factors, providing actionable insights into the circumstances under which accidents are most likely to occur. Utilizing advanced data visualization techniques, including interactive heatmaps and histograms, the project presents complex information in a clear and engaging manner to enhance understanding and foster data-driven decision-making.\nThe findings emphasize the critical role of environmental and behavioral factors in road safety and aim to support policymakers, researchers, and road users in designing effective interventions to reduce accidents and improve safety measures. This project underscores the importance of leveraging statistical data to promote evidence-based strategies for safer transportation systems.\nVisualizing Accident Severity Distribution\nThis interactive histogram presents a comprehensive analysis of road accident severity levels categorized as Life-Threatening, Significant, and Mild. Each category is visually distinguished using a specific color palette, with red denoting Life-Threatening accidents, orange representing Significant accidents, and green for Mild cases. The visualization provides an intuitive understanding of the frequency distribution of these severity levels, enabling researchers and policymakers to identify patterns and focus on mitigating the most critical accident types. The interactivity of the graph allows for an in-depth examination of accident counts, enhancing data-driven decision-making and supporting evidence-based road safety interventions.\n\n\n\n\n\n\nTemporal Analysis of Road Accidents by Time Bands\nThis interactive visualization categorizes road accidents into five time bands: “Night (Midnight to 5 AM),” “Morning Rush Hour,” “Daytime,” “Evening Rush Hour,” and “Night (8 PM to 11 PM)” using STATS20 guidance. The bar plot highlights accident frequencies with a gradient color scheme, showing the highest occurrences during “Daytime” and “Evening Rush Hour”.\nThese insights help identify high-risk periods, enabling policymakers and researchers to develop targeted road safety strategies. The interactive design allows for detailed exploration of accident patterns.\n\n\n\n\n\n\nAccidents by Weather and Light Conditions\nThis interactive heatmap analyzes the influence of weather and light conditions on road accidents, highlighting combinations like “Fine without high winds” and “Daylight” with the highest frequencies. A gradient color scale emphasizes accident intensity, with data labels providing exact counts. The visualization aids in identifying high-risk conditions to inform targeted safety measures.\n\n\n\n\n\n\nImpact of Weather Conditions on Road Accidents\nThis interactive bar plot presents the distribution of road accidents under various weather conditions, highlighting categories such as Fine without high winds Raining without high winds, and Fog or mist. The gradient color scale, ranging from light pink to deep red, emphasizes the frequency of accidents, with higher counts visually more prominent. Tooltips provide precise accident counts for each weather condition, enhancing the interpret ability of the data.\nThe visualization reveals that the majority of accidents occur under Fine without high winds, suggesting that favorable weather does not necessarily mitigate risk. Such insights are critical for policymakers and researchers to understand environmental influences on road safety and to develop targeted prevention strategies.\n\n\n\n\n\n\nAccidents by Road Surface Conditions\nThis interactive bar plot examines the distribution of road accidents across various surface conditions based on STATS19 classifications, such as Dry, Wet or damp, and Snow. Each condition is color-coded for clarity, with tooltips providing detailed accident counts for enhanced interpretability.\nThe analysis reveals that the majority of accidents occur on Dry surfaces, followed by Wet or damp conditions, while adverse surfaces like Flood and Mud show significantly lower frequencies. These findings emphasize the need to consider surface conditions when implementing road safety measures, particularly for common scenarios like wet or dry roads. The visualization supports data-driven strategies for reducing accidents under diverse environmental conditions.\n\n\n\n\n\n\nRandom Forest Model to find What combination of factors most strongly predicts accident severity? \nA Random Forest model was developed to classify accident severity based on features such as weather conditions, road surface conditions, number of vehicles, and urban or rural location. The model, trained on 100 decision trees, achieved robust classification with reasonable AUC values across all severity levels, as visualized in the ROC curves for each class.\nThe feature importance plot highlights “Weather Conditions” and “Road Surface Conditions” as the most significant predictors of accident severity, followed by “Number of Vehicles” and “Urban or Rural Area.” These insights provide valuable guidance for prioritizing interventions and refining predictive models to improve road safety outcomes. The analysis underscores the importance of environmental and contextual factors in accident severity classification.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logistic Regression Analysis of Accident Severity by Weather Conditions\nA multinomial logistic regression model was developed to examine the relationship between accident severity and weather conditions using a cleaned subset of the dataset. The dataset was partitioned into training (80%) and testing (20%) subsets to ensure robust evaluation. The model achieved convergence after 10 iterations, with residual deviance and AIC values of 100785 and 100793, respectively.\nThe confusion matrix revealed that the model performed well in classifying higher severity levels, achieving an overall accuracy of 76.06%. The coefficients indicate a positive association between weather conditions and accident severity, suggesting that as adverse weather conditions increase, the likelihood of severe accidents also rises. These findings underscore the critical role of weather in road safety and provide insights for preventive measures.\nROC Curve Analysis for Multinomial Logistic Regression Model\nThe ROC curve illustrates the predictive performance of the multinomial logistic regression model in classifying accident severity levels (Fatal, Serious, and Slight) based on weather conditions. One-vs-all ROC curves were generated for each class, with distinct color coding: red for Fatal, blue for Serious, and green for Slight.\nThe curves largely overlap with the diagonal reference line, indicating limited separation between true positive and false positive rates. As a more distanced ROC curve signifies better model performance, these results suggest the need for further feature refinement or model optimization to improve classification accuracy. The AUC values, though reasonable, highlight areas for potential enhancement in predictive capability.\n\n\n# weights:  9 (4 variable)\ninitial  value 91633.053773 \niter  10 value 50406.679444\nfinal  value 50392.493956 \nconverged\n\n\n\n\n\n\n\n\n\n\nLogistic Regression for Binary Accident Severity Classification to predict accident severity by rural vs uraban areas in certain weather conditions.\nA logistic regression model was employed to classify accident severity into binary categories: “Slight” (1) and “Fatal/Serious” (0), using features such as urban or rural area and weather conditions. The model showed a modest reduction in residual deviance (from 91810 to 91030), with an AIC of 91040, suggesting limited improvement over the null model.\nThe ROC curve yielded an AUC value of 0.5498, indicating the model’s predictive performance is slightly better than random chance. The negative coefficient for “urban_or_rural_area” suggests that accidents in urban areas are more likely to be classified as “Slight,” while the positive coefficient for “weather_conditions” implies a weak association with increased severity. Overall, the model demonstrates minimal predictive capability and requires additional features or refinement to achieve better classification accuracy and practical applicability.\n\n\n\n\n\n\n\n\n\nLasso Logistic Regression for Binary Classification of Accident Severity\nA Lasso logistic regression model was applied to classify accident severity into binary categories (“Slight” vs. “Fatal/Serious”) using features such as urban or rural area and weather conditions. The model employed cross-validation to identify the optimal regularization parameter (lambda.min), ensuring reduced overfitting and improved generalizability.\nThe ROC curve yielded an AUC of 0.5514, indicating a marginally better performance than random guessing. The close proximity of the ROC curve to the diagonal reference line suggests limited predictive power. While the model effectively reduces feature complexity, the low AUC highlights the need for additional predictive variables or refined feature engineering to improve classification accuracy and ensure practical applicability.\n\n\n\n\n\n\n\n\n\nLasso Logistic Regression for Predicting Accident Severity inclusing more variables\nA Lasso logistic regression model was implemented to classify accident severity into binary categories: “Severe” (1) and “Slight” (0). The model utilized key features such as road surface conditions, weather conditions, urban or rural area, and time of day. Cross-validation identified the optimal regularization parameter (lambda.min), ensuring feature selection and preventing over fitting.\nThe model achieved an AUC of 0.612, which is the highest among all models evaluated, as visualized through the ROC curve. This indicates improved predictive performance and better discriminatory power compared to earlier approaches. Feature coefficients highlighted the importance of road surface conditions and urban/rural areas as significant predictors. While the model demonstrates improved performance, further enhancements could refine its applicability for real-world scenarios."
  },
  {
    "objectID": "ResearchQuestions.html",
    "href": "ResearchQuestions.html",
    "title": "Final Project - Group 11",
    "section": "",
    "text": "RESEARCH QUESTIONS:\nResearch Question 1:\nWhat combination of factors most strongly predicts accident severity?\nModel: Random forests\nFeatures: Weather Conditions\nTarget Variable: Accident severity (categorized as Slight, Serious, and Fatal accidents)\nObjective: The number of vehicles involved is the strongest predictor of accident severity. Urban or rural area also impacts severity, likely due to differences in traffic patterns and road infrastructure. Road surface conditions and weather conditions play a smaller but notable role in influencing accident severity.\nInsights: Accidents with more vehicles tend to be more severe, as they involve higher chances of complex collisions and injuries. Urban areas may see higher severity due to dense traffic, while rural areas could have higher speeds leading to severe outcomes. Adverse weather and poor road surface conditions exacerbate accident severity, though their individual contributions are less than the number of vehicles or urban/rural classification.\nLimitations: The model underperforms for rare cases (fatal and serious accidents), likely due to imbalanced data. Additional factors (e.g., driver behavior, time of day, and speed) could improve predictions.\nResearch Question 2:\nCan the weather conditions predict accident severity?\nModel: Multinomial Logistic Regression\nFeatures: Weather Conditions\nTarget Variable: Accident severity (categorized as Slight, Serious, and Fatal accidents)\nObjective: To determine if weather conditions can predict accident severity.\nA Slight increase in weather condition values slightly raises the likelihood of higher accident severity. Coefficients for Severity 2 (Serious) and Severity 3 (Slight) suggest weather conditions play a modest role in predicting severity.\nResearch Question 3 :\nDoes Urban vs. Rural Areas affect accident severity under different weather conditions ?\nModel: Logistic Regression\nFeatures: Weather Conditions\nTarget Variable: Accident severity (categorized as Slight, Serious, and Fatal accidents)\nObjective:\nInfluence of Urban vs. Rural Areas:\nHow the location type (urban or rural) impacts the likelihood of slight accidents compared to fatal or serious ones.\nImpact of Weather:\nWhether weather conditions have a significant effect on the severity of accidents when combined with urban/rural classification.\n\n\nConcluded the analysis by continuing the Logistic Regression with Lasso.\nLogistic Regression Lasso has performed well with most significant variables predicting accident severity."
  }
]